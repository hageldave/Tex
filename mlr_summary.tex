% !TeX spellcheck = en_GB
\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{xfrac}
\usepackage{esvect}
\setlength{\parindent}{0pt}
\setlength{\parskip}{.36cm plus0.09cm minus0.09cm}
\usepackage[left=2.25cm, right=2.25cm, top=2.25cm, bottom=2.25cm]{geometry}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{esvect}
\usepackage{csquotes}
\usepackage{algpseudocode}

\newcommand{\lam}{\lambda}
\newcommand{\trans}{\quad\bigg|\;}
\newcommand{\Trans}{\quad\Bigg|\;}
% matrix shortcut
\newcommand{\mat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
% matrix shortcut with increased vertical spacing
\newcommand{\Mat}[1]{\begingroup
\renewcommand*{\arraystretch}{1.5}
\mat{#1}
\endgroup}
% matrix format but without braces (secretly a matrix)
\newcommand{\secretMat}[2][r]{\begin{matrix*}[#1] #2 \end{matrix*}}
% matrix format but without braces (secretly a matrix) with increased vertical spacing
\newcommand{\SecretMat}[2][r]{\begingroup
\renewcommand*{\arraystretch}{1.7}
\secretMat[#1]{#2}
\endgroup}
% shortcuts for df/dx stuff in derivative
\newcommand{\der}{\partial}
\newcommand{\deriv}[2]{\frac{\der #1}{\der #2}}
\newcommand{\eqns}[1]{\begin{flalign} #1 \end{flalign}}
\newcommand{\eqnsnn}[1]{\begin{flalign*} #1 \end{flalign*}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\dom}[1]{\mathbb{#1}}
\newcommand{\T}{^\top}
\newcommand{\equivalent}{\Leftrightarrow}
\newcommand{\mathtext}[1]{\quad\text{#1}\quad}
\newcommand{\with}{\mathtext{with}}
% make star '*' become \cdot (normal multiplication sign)
\mathcode`\*="8000
{\catcode`\*\active\gdef*{\cdot}}

\begin{document}
  \begin{center}
    \Large\textbf{Summary - Machine Learning}\\
    \vspace{0.25cm}
    \large{David H\"agele}, \normalsize{\today}
   \end{center}
   
\tableofcontents

\section{Machine Learning Briefly}
The goal of machine learning is to estimate or \enquote{learn} a mapping $f$ from one domain $\dom{X}$ to another $\dom{Y}$ by using a set of data points.
\eqnsnn{
f : \dom{X} \to \dom{Y} &&
}
In the field of supervised learning a data set $D$ usually contains observed data points of domain $\dom{X}$ and corresponding points of domain $\dom{Y}$. 
The objective is to find a mapping $f_\text{optimal}$ that minimizes an error $err$ between observed and estimated $y$.
\eqnsnn{&
D = \{ x_i , y_i \}\quad|\quad x_i\in\dom{X},~ y_i \in \dom{Y},~ i\in\dom{N}
&&\\&
err : \dom{Y}\times\dom{Y} \to \dom{R}
&&\\&
f_\text{optimal} = \argmin_f\;err(y,f(y))
&&}
In the field of unsupervised learning there are no corresponding observed points of domain $\dom{Y}$.
Optimality is not measured using an error to the observation, instead other objectives are to be fulfilled.

\section{Linear Regression}
One of the most basic algorithms for estimating a real valued function is linear regression.
Given a set of multivariate inputs $x_i \in \dom{R}^m$ and corresponding real valued outputs $y_i \in \dom{R}$ the following \enquote{loss} $L$ is to be minimized by a linear function $f_\beta:\dom{R}^m\to\dom{R}$.
\eqns{&
L(\beta) = \sum_{i=1}^n (\;y_i - f_\beta(x_i)\;)^2 
&&\\&
f_\beta(x) = x\T\beta \with\beta\in\dom{R}^m
&&}
This is the mean squared error and we want to minimize it using a linear function.
Lets call the matrix of transposed $x_i$'s $X$ and the vector of $y_i$'s $y$.
This allows for rewriting the loss as follows.
\eqnsnn{&
L(\beta) = (y-X\beta)\T(y-X\beta)
&&\\&
X = \mat{x_1\T\\\vdots\\x_n\T} \quad y = \mat{y_1\\\vdots\\y_n}
&&}
When minimizing we seek the roots of the derivative of the loss, by doing so we can derive the optimal solution for beta.
\eqnsnn{
\deriv{L(\beta)}{\beta} = 0 ~&\equivalent~ -(y-X\beta)\T X - (y-X\beta)\T X = -2*(y-X\beta)\T X = 0
&&\\&
\equivalent (y-X\beta)\T X = X\T y - X\T X\beta = 0
&&\\&
\equivalent X\T y = X\T X\beta
&&\\&
\equivalent (X\T X)^{-1}X\T y = \beta
&&
}
\subsection{Non-Linear Features}
Sadly the very simple linear regression algorithm only allows for linear functions to be estimated. 
Fortunately we can make things non-linear by defining a feature function which maps an input $x$ to feature space like so:
\eqnsnn{
\phi(x) \in \dom{R}^l \with l\in\dom{N}
&&}
For real valued $x\in\dom{R}$ we can create the quadratic feature vector using the following.
\eqnsnn{
\phi(x) = \mat{1&x&x^2}\T
&&}
When using linear regression on these features we end up with a function that is quadratic in $x$ but linear in features $\phi(x)$.
\eqnsnn{&
X = \mat{\phi(x_1)\T\\\vdots\\\phi(x_n)\T}
&&\\&
f_\beta(x) = \phi(x)\T\beta = \phi(x)_1*\beta_1 + \phi(x)_2*\beta_2 + \phi(x)_3*\beta_3 = \beta_1 + x\beta_2 + x^2\beta_3
&&
}
Even more sophisticated features could be using gaussian functions at the datapoints which then amounts to a radial basis function.
\eqnsnn{&
\phi(x) = \mat{1 & \phi_1(x) & \cdots & \phi_N(x)}\T \with \phi_i(x) = e^{-\frac{1}{2}||x_i-x||_2^2}
&&}

\subsection{Regularization}
Maybe you may have noticed that the linear regression algorithm may fail in case that $(X\T X)$ is not invertible.
To ensure invertibility a tiny quantity can be added to or subtracted from the diagonal of the matrix like so: $(X\T X - \lambda I)$.
Interestingly, the optimal solution to the following loss has this exact form.
\eqnsnn{
L(\beta) = (y&-X\beta)\T(y-X\beta) + \lambda*\beta\T\beta
&&\\
\deriv{L(\beta)}{\beta} = 0 &\equivalent -2(y-X\beta)\T X + 2\lambda*\beta\T = 0
&&\\
&\equivalent \lambda*\beta\T - (y-X\beta)\T X = 0
&&\\
&\equivalent \lambda*\beta - X\T y + X\T X\beta = 0 
&&\\
&\equivalent X\T y = X\T X\beta - \lambda*\beta = (X\T X - \lambda I) \beta
&&\\
&\equivalent (X\T X - \lambda I)^{-1}X\T y = \beta
&&} 
The loss function can be viewed as a combination of penalties for $\beta$.
The first penalty is the mean squared error from before which is large when $\beta$ cannot reproduce the samples $y$ well.
The second penalty is for $\beta$ of long lenghts, which means that short $\beta$ with small coefficients are preferred.
These two penalties probably are contradictive and the optimal beta will be somewhere in between the individual minima for the penalties.
This kind of regularization using $\lambda\beta\T\beta$ is called $L^2$ regularization and is a special case of the general Tikhonov regularization formulating constraints on $\beta$ as $\beta\T\Gamma\T\Gamma\beta$ with Tikhonov matrix $\Gamma$.
$\Gamma\beta$ is a linear mapping which can be chosen to also formulate special constraints on beta like differences between coefficients, or projections to different vector spaces.

When we use only half of the data set for \enquote{training} our model i.e. solving for $\beta$ we can check how well it reproduces the other half of the data set, which indicates how well the model generalizes.
While the model may be very accurate for the data used for training, it may be inaccurate for the data used for \enquote{validation} (the other half), which means that the model is \enquote{over fitted} to the training data and does not approximate the actually underlying function well.
In our case we can use regularization to allow the model to have a less perfect fit to the data when it instead fulfills the other objective of being small.
The degree to which the model can violate the mean squared error depends on $\lambda$.

\subsection{Cross Validation}
In order to find the perfect regularization parameter $\lambda$ in order for our model to generalize well we need to try some out.
This can be done using cross validation.
In cross validation, we divide our data set into $K$ euqually sized parts (randomly), then we train our model using K-1 parts of the data and validate the model by computing the mean suared error using the missing part.
This is done k times so that each part has been used for validation once and the errors are averaged.
The following pseudo code should make the algorithm clear.
\begin{algorithmic}[1]
\State Split data $D$ into $K$ equally sized parts $\{D_1,\dots,D_K\}$ randomly
\For{$k=1\dots K$}
\State train on data $D_{\backslash D_k}$ yielding $\beta_i$
\State calculate MSE using $D_k$ ~ $err_k = L^{MSE}(\beta_k,D_k)$
\EndFor
\State calculate total MSE as $err=\frac{1}{K}\sum_{k=1}^K \frac{err_k}{|D_k|}$ 
\end{algorithmic}
The total error will be a measure for generalization of the model, using this measure an optimal $\lambda$ can be found by trying different $\lambda$ and adaptively tightening the search space.
For the start it may be useful to try lambdas of different orders of magnitude first.


\section{Support Vector Machine}
Imagine a binary classification problem where the data set consists of multivariate inputs $x_i \in \dom{R}^M$ and discrete outputs $y_i \in \{-1,1\}$ ($i = \{1,\dots,N\}$) which means that a point $x$ may either belong to class $1$ or $-1$.
For this kind of problem we want to find a function that can separate the points of one class from another.
In the 2 dimensional case, a simple separator would be a straight line and a point can either be on one or the other side of it.
For higher dimensional $x$ the separator would be a hyperplane.

To get used to the plane stuff again, lets review the point-normal form of a plane.
In the simplest case where the plane intersects the origin, we can check if a point is in the plane by projecting its position vector $x$ onto the plane normal $n$. When $x$ is orthogonal to $n$ the projection $x\T n = 0$ and $x$ is in the nullspace of $n$ that is the plane.
When the plane does not intersect the origin we first need to translate the coordinatesystem so that it does.
We can do this by subtarcting a support vector of the plane $p_0$ (a position vector of a point that is part of the plane) from $x$, resulting in the check $(x-p_0)\T n = 0$ which can be rewritten as follows.
\eqnsnn{&
(x-p_0)\T n = 0 \equivalent x\T n + d = 0 \quad\text{with constant}\quad d = -p_0\T n
&&}
If $n$ is of unit length, then the distance of $x$ to the plane is given by the projection onto the normal $dist=x\T n + d$.
When not normalized then the distance is given by the following.
\eqnsnn{&
dist = (x-p_0)\T \frac{n}{|n|} = \frac{x\T n}{|n|} - \frac{p_0\T n}{|n|} = \frac{x\T n + d}{|n|}
&&}
This means that the projection on the normal is the distance to the plane scaled by the normals length $x\T n + d = dist*|n|$.
For a fixed projection value, this means, the smaller the normal the greater the distance.
In fact for a projection of value $1$ the distance is $\frac{1}{|n|}$.
We want the margin, that is the \enquote{width} of the separator, to be maximal.
If we require projections to be at least $1$ (or $-1$ depending on the side) the margin is $\frac{1}{2*|n|}$. 

Lets formulate the separation objective as a constrained optimization problem with an inequality constraint, from which we can set up a Lagrangian.
\eqnsnn{&
\hat{n} = \argmin_{n} \frac{1}{2}|n|^2 \quad\text{subject to}\quad y_i(x_i\T n + d) \geq 1
&&\\&
\mathcal{L}(n,d,\lambda) = \frac{1}{2}\sum_{j=1}^M (n_j)^2 - \sum_{i=1}^N \lambda_i *(y_i(x_i\T n + d)-1)
&&}
When the data does not allow for a linear separation of the datapoints, then this problem cannot be solved because there is no feasible region.
But we can modify the problem in order to allow points to be on the wrong side of the plane at a cost.
For this a new variable $\xi_i\geq 0$ is introduced which measures by how much the inequality constrained has been violated and sanitizes the constraint so that it becomes feasible.
Of course, we want the amount of violations to be minimal, so we introduce a penalty on large $\xi$ which we control by a weighting parameter $C$.
\eqnsnn{&
\hat{n} = \argmin_{n} \frac{1}{2}|n|^2 + C*\sum_{i=1}^N\xi_i \quad\text{subject to}\quad y_i(x_i\T n + d) \geq 1-\xi_i ~\text{and}~ \xi_i \geq 0 
&&\\&
\mathcal{L}(n,d,\lambda,\mu) = \frac{1}{2}\sum_{j=1}^M (n_j)^2  + C*\sum_{i=1}^N \xi_i - \sum_{i=1}^N \lambda_i *(y_i(x_i\T n + d)-1+\xi_i) - \sum_{i=1}^n \mu_i*\xi_i
&&}
When solving the dual problem (which is always convex), we get the optimal normal and classifier as
\eqnsnn{&
\hat{n} = \sum_{i=1}^N \hat{\lambda}_iy_ix_i
&&\\&
f(x) = \text{sign}(x\T \hat{n} + d)
&&\\&
\hat{\lambda} = \argmax_\lambda \sum_{i=1}^N\lambda_i - \frac{1}{2}\sum_{i=1}^N \sum_{k=1}^N \lambda_i \lambda_k y_i y_k x_i\T x_k
\quad\text{subject to}\quad \sum_{i=1}^N \lambda_i y_i = 0 ~\text{and}~ C \geq \lambda_i \geq 0
&&}
Since $\lambda_i$ are the lagrangian multipliers of the inequality constraint they are either inactive ($=0$) if the corresponding point is correctly classified and outside the margin, or active ($\neq 0$) if the corresponding point violates the constraint. 
So all active $x_i$ contribute to the solution and are called the support vectors.

\section{Logistic Regression}
We already discussed one classifier, the support vector machine, that is a binary classifier.
In this section we will take a look at a different approach to classification.
As usual we have multivariate $x_i \in \dom{R}^M$ as inputs which are labeled with a discrete variable $y_i \in \dom{Y}=\{Y_1,\dots,Y_l\}$ ($i = {1,\dots,N}$).
This time we want to know the probability of a class $y$ given that we observed some $x$ which is denoted by $p(y|x)$.

To estimate the probabilities we make use of a discriminative function $f(x,y):\dom{R}^M \times \dom{Y} \to \dom{R}$ that we have to learn.
The discriminative function has to have a high value when $x$ is likely to be of class $y$, and a low value when it's not so that $y(x) = \argmax_y f(x,y)$ where $y(x)$ is the correct class for $x$.

Similar to the Boltzmann distribution $p(x) = e^{-E(x)}$ where $E(x)$ is an energy (or error), the probabilities are calculated as:
\eqnsnn{&
p(y|x) = \frac{e^{f(x,y)}}{Z} \mathtext{with normalization} Z = \sum_{y'\in\dom{Y}} e^{f(x,y')}
&&}

Now that we set up the basic framework, we need to define the discriminative function.
Lets formulate it as a linear function in features (similar to linear regression).
\eqnsnn{&
f(x,y) = \phi(x,y)\T\beta \with \phi(x,y) = \mat{\phi(x)[y=Y_1] \\ \phi(x)[y=Y_2] \\ \vdots \\ \phi(x)[y=Y_l]}
&&}
The $[cond]$ operator is $1$ if the condition is true, otherwise $0$.

The objective of our model is to maximize the likelihood $p(y_i|x_i)$ for each $(x_i,y_i)$, but instead it could also maximize the log likelihood $\ln(p(y_i|x_i))$ since the logarithm is a monotonic increasing function.
Also, in machine learning we rather like to minimize stuff, so lets minimize the neg-log-likelihood by minimizing the following loss.
\eqnsnn{
L(\beta) &= -\sum_{i=1}^N \ln(\;p(y_i|x_i)\;) 
= - \sum_{i=1}^N\Big( \ln\big( e^{f(x_i,y_i)}\big) - \ln\big(\sum_{y' \in \dom{Y}} e^{f(x_i,y')}\big) \Big)
&&\\
&= - \sum_{i=1}^N\Big( \ln\big( e^{f(x_i,y_i)}\big)\Big) +\sum_{i=1}^N\Big( \ln\big(\sum_{y' \in \dom{Y}} e^{f(x_i,y')}\big) \Big) = -A(\beta) + B(\beta)
&&
}
The derivative of the loss is then calculated from the two separated sums $A(\beta)$ and $B(\beta)$:
\eqnsnn{
\deriv{L(\beta)}{\beta} &= -\deriv{A(\beta)}{\beta} + \deriv{B(\beta)}{\beta}
&&\\
\deriv{A(\beta)}{\beta} &= \sum_{i=1}^N \deriv{f(x_i,y_i)}{\beta} = \sum_{i=1}^N \phi(x_i,y_i)\T
&&\\
\deriv{B(\beta)}{\beta} &= \sum_{i=1}^N \deriv{}{\beta} \ln\big(\sum_{y' \in \dom{Y}} e^{f(x_i,y')}\big) 
&&\\
&= \sum_{i=1}^N \frac{1}{\sum_{y' \in \dom{Y}} e^{f(x_i,y')}} * \deriv{}{\beta} \sum_{y' \in \dom{Y}} e^{f(x_i,y')}
&&\\
&=\sum_{i=1}^N \frac{1}{Z} * \sum_{y' \in \dom{Y}} e^{f(x_i,y')}*\phi(x_i,y')\T
&&\\
&=\sum_{i=1}^N \sum_{y' \in \dom{Y}} 
\frac{e^{f(x_i,y')}}{Z} * \phi(x_i,y')\T
&&\\
&=\sum_{i=1}^N \sum_{y' \in \dom{Y}} p(y'|x_i) * \phi(x_i,y')\T
&&\\
\Big(\deriv{L(\beta)}{\beta}\Big)\T &= \nabla_\beta L(\beta) = \sum_{i=1}^N \Big( \sum_{y' \in \dom{Y}} p(y'|x_i) * \phi(x_i,y') \Big) - \phi(x_i,y_i)
&&}

As can be seen, the gradient of the loss is not linear in $\beta$ because $\beta$ sits inside of the non-linear calculation of $p(y'|x_i)$.
So unfortunately we cannot analytically derive the optimal $\beta$, but instead we can use the Newton algorithm to iteratively get there.

\subsection{Newton Algorithm}
The newton algortihm can be derived from a taylor expansion of a function.
Lets review the taylor expanson first, by writing down a univariate $(n-1)$ order taylor of $f$ around $x$ with step size $h$.
\eqnsnn{&
f(x+h) \approx f(x) + \frac{f'(x)*h}{1!} + \frac{f''(x)*h^2}{2!} + \frac{f^3(x)*h^3}{3!} + \dots + \mathcal{O}(h^n)
&&}
From this, the forward difference can be derived (which is not what we want, but shows how neat Taylor is).
\eqnsnn{&
f(x+h) \approx f(x) + f'(x)*h + \mathcal{O}(h^2) \;\equivalent\; \frac{f(x+h)-f(x)}{h} \approx f'(x) + \mathcal{O}(h^2)
&&}

Lets do something else, lets set a first order Taylor to zero (root finding):
\eqnsnn{
f(x_{n+1}) = f(x_n+h) &\approx f(x_n) + f'(x_n)*h + \mathcal{O}(h^2)
&&\\
0 &\approx f(x_n) + f'(x_n)*h
&&\\
0 &\approx \frac{f(x_n)}{f'(x_n)} + h
&&\\
h &\approx - \frac{f(x_n)}{f'(x_n)}
&&}
Here we derived the stepsize $h$ of a newton step, which we can use to update the position $x_n$ in order to get $x_{n+1} = x_n+h$. 
When iterating this newton step, we approach a root of $f(x)$.

In our case, we want to find a root of the gradient $\nabla L$, lets again formulate a Taylor expansion, this time multivariate second order with step $\delta$ and Hessian $\mathcal{H}_L$.
\eqnsnn{
L(\beta_{n+1}) = L(\beta_n + \delta) &\approx L(\beta_n) + \nabla L(\beta_n)\T \delta + \frac1 2 \delta\T \mathcal{H}_L(\beta_n)\delta + \mathcal{O}(|\delta|^3)
&&}
The derivative of this taylor with respect to the step $\delta$ yields an approximation of the derivative of $L$ around $\beta_n$. To find a root of it, we set it to zero.
\eqnsnn{
0 = \deriv{L(\beta_n+\delta)}{\delta} &\approx \nabla L(\beta_n)\T + \delta\T \mathcal{H}_L(\beta_n)
&&\\
0 &\approx \nabla L(\beta_n)\T[\mathcal{H}_L(\beta_n)]^{-1} + \delta\T \mathcal{H}_L(\beta_n)[\mathcal{H}_L(\beta_n)]^{-1} = \nabla L(\beta_n)\T[\mathcal{H}_L(\beta_n)]^{-1} + \delta\T
&&\\
\delta &\approx -[\mathcal{H}_L(\beta_n)]^{-1}\nabla L(\beta_n)
&&}
This is the newton step we can use to iteratively work our way to the optimal $\beta$ using 
\\$\beta_{n+1} = \beta_n+\delta$.
What is missing in order to optimize $\beta$ for logistic regression is the Hessian of the loss $\mathcal{H}_L(\beta) = \deriv{}{\beta}\nabla L(\beta)$, which is left to the reader to derive.


\section{Neural Networks}
Neural networks currently are the very hype in many fields of inteligent systems, probably because they are so flexible.
But lets not get carried away and start by defining a regression problem.
Given data $D=\{x_i,y_i\}_{i=1}^N$ with multivariate $x_i \in \dom{R}^M$ and scalar $y\in\dom{R}$ we want to estimate again as in linear regression $y=f(x)$.
For this, we model the function $f$ as follows:
\eqnsnn{&
f(x) = z_{L+1} = W_L*x_L \with x_l = \sigma(z_l) \;\;l\in\{2,\dots,L\} \mathtext{,} \sigma(z) = \frac{1}{1+e^{-z}} \mathtext{and} x_1 = x
&&}
In this equation we have $l$ matrices $W_1 \dots W_l$ with dimensions $W_1^{d(2)\times M}\; W_2^{d(3)\times d(2)}\dots W_l^{1\times d(l)}$. 
The sigmoid function $\sigma$ takes the role of the \enquote{activation function} which is an analogy to neurons in a brain which can propagate an action potential based on the magnitude of the signal.

Lets get more intuition on this from a simple example.
Suppose $x \in \dom{R}^3$ and $y \in \dom{R}^1$ and we have three matrices $W_1\;W_2\;W_3$.
To be able to input our $x$ into the neural net we have to choose the first dimension of $W_1$ to be $3$ and the last dimension of $W_3$ to be $1$ in order to map to $\dom{R}^1$. The other dimensions are left to be choosen freely. Lets choose $W_2^{10\times 10}$ which implies $W_1^{10 \times 3}$ and $W_3^{1 \times 10}$.
Our function is then:
\eqnsnn{&
f(x) = W_3\;\sigma\Big(W_2\;\sigma(W_1x)\Big)
&&}
This algorithm is called the forward propagation of $x$ through the neural network.
The unknowns in this equation are the entries of the matrices, the so called weights.
In order to optimize the weights we need to define a cost function $c(f,y)$ (a loss) for the prediction $f$.
In the literature, we can find many different cost functions that can be used, we'll stick to the well known mean squared error.
\eqnsnn{&
c(f,y) = (f-y)^2 \mathtext{so that} C_\text{MSE} = \frac1n \sum_{i=1}^N c(f(x_i),y_i)
&&\\&
\deriv{\;c(f,y)}{f} = 2(f-y)\T \mathtext{if $f$ and $y$ were vectors, in our case we can ommit the transpose}
&&}
Lets call $\deriv{\;c(f,y)}{f} = \delta_{L+1}$ as it is the derivative of the loss with repect to $z_{L+1}$ (in our case $\delta_4$ the derivative of $f(x) = z_4=W_3\;x_3$).
Due to the chain rule the derivatives for other layers are recursively calculated like this:
\eqnsnn{&
\delta_l = \deriv{c}{z_l} = \deriv{c}{z_{l+1}} * \deriv{z_{l+1}}{x_l}*\deriv{x_l}{z_l}
&&}
The derivative of the of the loss with respect to a specific weight of a matrix $W_{l,ij}$ is given by:
\eqnsnn{&
\deriv{c}{W_{l,ij}} = \deriv{c}{z_{l+1,i}}*\deriv{z_{l+1,i}}{W_{l,ij}}
&&}

To optimize the weights of the matrices, we feed the neural net all of our data samples $x_i$ and collect the losses $c(f(x_i),y_i)$.
We then backpropagate the losses through the net, which means we collect the derivatives of the losss for all weights and sum corresponding derivatives up forming a total gradient for a specific weight.
The weights are then updated using a gradient descend step which reads $W_{l,ij} = W_{l,ij} - \alpha \deriv{c}{W_{l,ij}}$ with learning rate $\alpha$ (probably $\alpha=0.1$).
And then we iterate forward and backward propagation.


\section{PCA}
Principle Component Analysis (PCA) is a method for dimensionality reduction.
This may be useful when you have very high dimensional data, but want to reduce the amount of data you want to process in order to learn a function.
In this case you may want to get a few of the \enquote{most important} parts of your highdimensional data point, or lets say a low dimensional datapoint which encodes the most important aspects of your high dimensional point.

A way to do that, is to find a projection of our data $x_i \in \dom R^M$ onto $z_i \in \dom R^P$ where $P \leq M$.
Lets look at the projection from the other side, lets say we transform $z$ to $x$ using the following, where $V_P$ is the projection matrix and $\mu$ is a vector accounting for a translation in $\dom R^M$.
\eqnsnn{&
x \approx V_P z + \mu
&&\\&
err = \sum_{i=1}^N || V_P z_i + \mu - x_i ||^2
&&}
The error $err$ measures the mean squared error of the transformation of $z$ to $x$.
Using this, we can derive the optimal transformation of $x$ to $z$ when minimizing with respect to $z$.
\eqnsnn{
\hat{z}_{1:N} &= \argmin_{z_{1:N}} \sum_{i=1}^N || V_P z_i + \mu - x_i ||^2
&&\\
0 &= \deriv{}{z_i}\sum_{i=1}^N || V_P z_i + \mu - x_i ||^2 = \deriv{}{z_i} (V_P z_i + \mu - x_i)\T(V_P z_i + \mu - x_i)
&&\\
 &= 2*(V_P z_i + \mu - x_i)\T V_P
&&\\
\equivalent 0 &= V_P\T V_P z_i + V_P\T(\mu-x_i)
&&}
If we constrain $V_p$ to be orthonormal (composed of orthonormal vectors), then $V_P\T V_P = I$ and this simplifies to the follownig:
\eqnsnn{&
z_i = V_P\T(x_i-\mu)
&&}
Doing the same for $\mu$ yields the following:
\eqnsnn{
\hat{\mu} &= \argmin_{\mu} \sum_{i=1}^N || V_P z_i + \mu - x_i ||^2
&&\\
0 &= \deriv{}{\mu} \sum_{i=1}^N (V_P z_i + \mu - x_i)\T(V_P z_i + \mu - x_i)
 = 2* \sum_{i=1}^N (V_P z_i + \mu - x_i)\T 
&&\\
\equivalent 0 &= N*\mu*\sum_{i=1}^N (V_P z_i - x_i)
&&\\
\equivalent \mu &= \frac1N\sum_{i=1}^N V_Pz_i - \frac1N\sum_{i=1}^N x_i
&&}
So $\mu$ is the difference of the expeted value of the projected $z_i$ and the expected value $x_i$.
Lets assume that $E[z] = 0$ because that would be nice, and we don't care about the expected value of $z$ as it is unrelated to what we try to achieve. 
Due to the linearity of the expected value the projection of $z$ also has $0$ as expected value.
\eqnsnn{&
\mu = \frac1N\sum_{i=1}^N x_i = E[x]
&&}

So the projection $z_i = V_P\T(x_i-\mu)$ is the projecting the \enquote{centered} $x_i$.
For the derivation of $V_P$ lets assume that $x_i$ are already centered ($E(x)=0$).
What we want to minimize is the projection error, which we can estimate by back projecting the projection of $x_i$ and comparing it with the initial $x_i$.
\eqnsnn {&
\hat{V}_P = \argmin_{V_P} \sum_{i=1}^N ||x_i- V_PV_P\T x_i||^2 
&&}
This time we will derive the optimum by observing the properties of the equation instead of root finding in the derivative.
Lets reformulate the loss a bit.
\eqnsnn {
L(V_P) &= \sum_{i=1}^N (x_i- V_PV_P\T x_i)\T(x_i- V_PV_P\T x_i)
&&\\
&= \sum_{i=1}^N x_i\T x_i - 2 x_i\T V_P V_P\T x_i 
+ x_i\T V_P V_P\T V_P V_P\T x_i
&&\\
& = \sum_{i=1}^N x_i\T x_i - 2x_i\T V_P V_P\T x_i  + x_i\T V_P V_P\T x_i
&&\\
&= \sum_{i=1}^N x_i\T x_i - x_i\T V_P V_P\T x_i
&&\\
&= C - \sum_{i=1}^N x_i\T V_P V_P\T x_i \with C = \sum_{i=1}^N x_i\T x_i
&&\\
&= C - \sum_{i=1}^N || V_P\T x_i ||^2 \mathtext{($V_P\T$ composed of row vectors $v_j\T$)}
&&\\
&= C - \sum_{i=1}^N \sum_{j=1}^P (v_j\T x_i)*(v_j\T x_i)
 \;=\; C - \sum_{j=1}^P \sum_{i=1}^N  (x_i\T v_j)*(x_i\T v_j)
&&\\
&= C - \sum_{j=1}^P ||X v_j||^2 \mathtext{($X$ composed of row vectors $x_i\T$)}
&&\\
&= C - \sum_{j=1}^P v_j\T X\T X v_j
&&}
So whatwe can see from this, is that the loss will be minimal if we maximize $\sum_{j=1}^P v_j\T X\T X v_j$.
We could again find the root of its derivative but, it will be more fun with some clever factorization of $X\T X$, which is the Eigendecomposition. 
Since $X\T X$ is a symmetric matrix, its Eigendecomposition is
\eqnsnn{&
X\T X = Q\Lambda Q\T \mathtext{with orthonormal} Q \mathtext{and diagonal} \Lambda
&&\\
& \argmax_{v_{1:P}} \sum_{j=1}^P v_j\T Q\Lambda Q\T v_j
&&} 
Suppose the eigenvlus $\lambda_1,\dots,\lambda_M$ are sorted in value descending order on the diagonal of $\Lambda$.
To maximize the the above expression, we need to choose $v_{1:P}$ to be the first $P$ eigenvectors $q_i$ corresponding to the $P$ largest eigenvalues. 
If we could, we would choose all $v_j$ to be the first eigenvector, as this would truly yield the maximum, but due to our constraint of $v_j$s being orthonormal to each other we can't do that.

One last thing to be mentioned before moving on is, that instead of using the Eigendecomposition of $X\T X$ to get the optimal $V_P$ we can as well use the singular value decomposition (SVD) of $X$ to get there:
\eqnsnn{&
X = USW\T \mathtext{with orthonormal} U,\;W \mathtext{and diagonal} S
&&\\&
X\T X = (USW\T)\T (USW\T) = W S\T U\T USW\T = WS^2W\T = Q\Lambda Q\T \implies W = Q \quad S^2=\Lambda
&&}








\end{document}